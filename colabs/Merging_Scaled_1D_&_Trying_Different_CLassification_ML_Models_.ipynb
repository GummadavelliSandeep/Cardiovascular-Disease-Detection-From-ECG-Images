{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshavinash94/Cardiovascular-Detection-using-ECG-images/blob/main/Merging_Scaled_1D_%26_Trying_Different_CLassification_ML_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIqvxE_BKjV6"
      },
      "source": [
        "### IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C82bu1OsKnfC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os \n",
        "from natsort import natsorted\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXx7SoC7qyRv"
      },
      "source": [
        "### **WORKING ON COMBING MULTIPLE LEAD FILES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sI0sFzAqPP8g"
      },
      "outputs": [],
      "source": [
        "#creating list to store file_names\n",
        "NORMAL_=[]\n",
        "MI_=[]\n",
        "PMI_=[]\n",
        "HB_=[]\n",
        "\n",
        "normal = 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\NORMAL'\n",
        "abnormal = 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\AHB'\n",
        "MI = 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\MI'\n",
        "MI_history = 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\PM'\n",
        "\n",
        "Types_ECG = {'normal':normal,'Abnormal_hear_beat':abnormal,'MI':MI,'History_MI':MI_history}\n",
        "\n",
        "for types,folder in Types_ECG.items():\n",
        "  for files in os.listdir(folder):\n",
        "    if types=='normal':\n",
        "      NORMAL_.append(files)\n",
        "    elif types=='Abnormal_hear_beat':\n",
        "      HB_.append(files)\n",
        "    elif types=='MI':\n",
        "      MI_.append(files)\n",
        "    elif types=='History_MI':\n",
        "      PMI_.append(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJL9qAFSUOsN",
        "outputId": "9a4817f5-ae50-4aaf-96fe-8cc945289ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaled_data_1D_1.csv',\n",
              " 'scaled_data_1D_2.csv',\n",
              " 'scaled_data_1D_3.csv',\n",
              " 'scaled_data_1D_4.csv',\n",
              " 'scaled_data_1D_5.csv',\n",
              " 'scaled_data_1D_6.csv',\n",
              " 'scaled_data_1D_7.csv',\n",
              " 'scaled_data_1D_8.csv',\n",
              " 'scaled_data_1D_9.csv',\n",
              " 'scaled_data_1D_10.csv',\n",
              " 'scaled_data_1D_11.csv',\n",
              " 'scaled_data_1D_12.csv',\n",
              " 'scaled_data_1D_13.csv']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NORMAL_ = natsorted(NORMAL_)\n",
        "NORMAL_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRbNR2HEUkvU",
        "outputId": "0cd461c0-5fe2-463a-cbf5-6325662d9011"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaled_data_1D_1.csv',\n",
              " 'scaled_data_1D_2.csv',\n",
              " 'scaled_data_1D_3.csv',\n",
              " 'scaled_data_1D_4.csv',\n",
              " 'scaled_data_1D_5.csv',\n",
              " 'scaled_data_1D_6.csv',\n",
              " 'scaled_data_1D_7.csv',\n",
              " 'scaled_data_1D_8.csv',\n",
              " 'scaled_data_1D_9.csv',\n",
              " 'scaled_data_1D_10.csv',\n",
              " 'scaled_data_1D_11.csv',\n",
              " 'scaled_data_1D_12.csv',\n",
              " 'scaled_data_1D_13.csv']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MI_ = natsorted(MI_)\n",
        "MI_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4LMfTU3UuCM",
        "outputId": "0c127e8e-62b7-4015-e3ee-a34860e4f5ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaled_data_1D_1.csv',\n",
              " 'scaled_data_1D_2.csv',\n",
              " 'scaled_data_1D_3.csv',\n",
              " 'scaled_data_1D_4.csv',\n",
              " 'scaled_data_1D_5.csv',\n",
              " 'scaled_data_1D_6.csv',\n",
              " 'scaled_data_1D_7.csv',\n",
              " 'scaled_data_1D_8.csv',\n",
              " 'scaled_data_1D_9.csv',\n",
              " 'scaled_data_1D_10.csv',\n",
              " 'scaled_data_1D_11.csv',\n",
              " 'scaled_data_1D_12.csv',\n",
              " 'scaled_data_1D_13.csv']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PMI_ = natsorted(PMI_)\n",
        "PMI_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OinhdMKtU5mf",
        "outputId": "52dd393b-76c2-44a9-e64e-4d428380ed8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['scaled_data_1D_1.csv',\n",
              " 'scaled_data_1D_2.csv',\n",
              " 'scaled_data_1D_3.csv',\n",
              " 'scaled_data_1D_4.csv',\n",
              " 'scaled_data_1D_5.csv',\n",
              " 'scaled_data_1D_6.csv',\n",
              " 'scaled_data_1D_7.csv',\n",
              " 'scaled_data_1D_8.csv',\n",
              " 'scaled_data_1D_9.csv',\n",
              " 'scaled_data_1D_10.csv',\n",
              " 'scaled_data_1D_11.csv',\n",
              " 'scaled_data_1D_12.csv',\n",
              " 'scaled_data_1D_13.csv']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "HB_ = natsorted(HB_)\n",
        "HB_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T4Ay50ch6Ru"
      },
      "source": [
        "#### **COMBINED CSV OF EACH LEAD(1-12) FROM ALL IMAGES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ot65BTX2VF7Y"
      },
      "outputs": [],
      "source": [
        "#loop over and create combined csv files for each leads.\n",
        "for x in range(len(MI_)):\n",
        "  df1=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\NORMAL\\\\{}'.format(NORMAL_[x]))\n",
        "  df2=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\AHB\\\\{}'.format(HB_[x]))\n",
        "  df3=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\MI\\\\{}'.format(MI_[x]))\n",
        "  df4=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\preprocessed_1d\\\\PM\\\\{}'.format(PMI_[x]))\n",
        "  final_df = pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
        "  final_df.to_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\Combined1d_csv\\\\Combined_IDLead_{}.csv'.format(x+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyTYwqJSX8Vh",
        "outputId": "b1c922e9-83f4-4d31-e5cd-d5b18a78acf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['No', 'HB', 'MI', 'PM'], dtype=object)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#now reading just lead1\n",
        "df=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\Combined1d_csv\\\\Combined_IDLead_1.csv')\n",
        "df['Target'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nbSKpUc2angF"
      },
      "outputs": [],
      "source": [
        "df.drop(columns=['Unnamed: 0'],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "fX_uYtekhL_z",
        "outputId": "a6d14a8e-a75f-4b1b-f39f-7830cba4c581"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.728449</td>\n",
              "      <td>0.680755</td>\n",
              "      <td>0.619010</td>\n",
              "      <td>0.645367</td>\n",
              "      <td>0.681570</td>\n",
              "      <td>0.732488</td>\n",
              "      <td>0.758448</td>\n",
              "      <td>0.750660</td>\n",
              "      <td>0.728282</td>\n",
              "      <td>0.707928</td>\n",
              "      <td>...</td>\n",
              "      <td>0.637260</td>\n",
              "      <td>0.664539</td>\n",
              "      <td>0.667226</td>\n",
              "      <td>0.637064</td>\n",
              "      <td>0.593287</td>\n",
              "      <td>0.545503</td>\n",
              "      <td>0.515049</td>\n",
              "      <td>0.563257</td>\n",
              "      <td>0.633581</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.957972</td>\n",
              "      <td>0.950695</td>\n",
              "      <td>0.941024</td>\n",
              "      <td>0.930501</td>\n",
              "      <td>0.913601</td>\n",
              "      <td>0.892244</td>\n",
              "      <td>0.868016</td>\n",
              "      <td>0.855127</td>\n",
              "      <td>0.835307</td>\n",
              "      <td>0.798640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.778790</td>\n",
              "      <td>0.806883</td>\n",
              "      <td>0.818640</td>\n",
              "      <td>0.842472</td>\n",
              "      <td>0.866740</td>\n",
              "      <td>0.884152</td>\n",
              "      <td>0.897196</td>\n",
              "      <td>0.911293</td>\n",
              "      <td>0.922903</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.611084</td>\n",
              "      <td>0.661575</td>\n",
              "      <td>0.695790</td>\n",
              "      <td>0.741113</td>\n",
              "      <td>0.716666</td>\n",
              "      <td>0.595794</td>\n",
              "      <td>0.425022</td>\n",
              "      <td>0.286457</td>\n",
              "      <td>0.425022</td>\n",
              "      <td>0.611384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042690</td>\n",
              "      <td>0.165850</td>\n",
              "      <td>0.363445</td>\n",
              "      <td>0.549460</td>\n",
              "      <td>0.539346</td>\n",
              "      <td>0.522272</td>\n",
              "      <td>0.491668</td>\n",
              "      <td>0.454949</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.839213</td>\n",
              "      <td>0.861690</td>\n",
              "      <td>0.866457</td>\n",
              "      <td>0.865756</td>\n",
              "      <td>0.855027</td>\n",
              "      <td>0.855606</td>\n",
              "      <td>0.845561</td>\n",
              "      <td>0.843187</td>\n",
              "      <td>0.846784</td>\n",
              "      <td>0.824438</td>\n",
              "      <td>...</td>\n",
              "      <td>0.789156</td>\n",
              "      <td>0.793622</td>\n",
              "      <td>0.787665</td>\n",
              "      <td>0.794515</td>\n",
              "      <td>0.796739</td>\n",
              "      <td>0.804063</td>\n",
              "      <td>0.809944</td>\n",
              "      <td>0.801814</td>\n",
              "      <td>0.777322</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.917753</td>\n",
              "      <td>0.924369</td>\n",
              "      <td>0.873765</td>\n",
              "      <td>0.791381</td>\n",
              "      <td>0.699513</td>\n",
              "      <td>0.604927</td>\n",
              "      <td>0.500312</td>\n",
              "      <td>0.446012</td>\n",
              "      <td>0.528910</td>\n",
              "      <td>0.634068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.200676</td>\n",
              "      <td>0.300147</td>\n",
              "      <td>0.407225</td>\n",
              "      <td>0.507346</td>\n",
              "      <td>0.605953</td>\n",
              "      <td>0.699309</td>\n",
              "      <td>0.790334</td>\n",
              "      <td>0.856593</td>\n",
              "      <td>0.849957</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>0.874246</td>\n",
              "      <td>0.877014</td>\n",
              "      <td>0.864280</td>\n",
              "      <td>0.860505</td>\n",
              "      <td>0.871349</td>\n",
              "      <td>0.912404</td>\n",
              "      <td>0.958148</td>\n",
              "      <td>0.977826</td>\n",
              "      <td>0.956314</td>\n",
              "      <td>0.926773</td>\n",
              "      <td>...</td>\n",
              "      <td>0.908312</td>\n",
              "      <td>0.926328</td>\n",
              "      <td>0.898749</td>\n",
              "      <td>0.855709</td>\n",
              "      <td>0.823132</td>\n",
              "      <td>0.815458</td>\n",
              "      <td>0.818083</td>\n",
              "      <td>0.829300</td>\n",
              "      <td>0.822382</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>0.829815</td>\n",
              "      <td>0.832084</td>\n",
              "      <td>0.852396</td>\n",
              "      <td>0.909665</td>\n",
              "      <td>0.988242</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.923323</td>\n",
              "      <td>0.821865</td>\n",
              "      <td>0.721302</td>\n",
              "      <td>0.612039</td>\n",
              "      <td>...</td>\n",
              "      <td>0.429721</td>\n",
              "      <td>0.531567</td>\n",
              "      <td>0.642137</td>\n",
              "      <td>0.742063</td>\n",
              "      <td>0.833042</td>\n",
              "      <td>0.814867</td>\n",
              "      <td>0.777622</td>\n",
              "      <td>0.760714</td>\n",
              "      <td>0.759294</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>0.469048</td>\n",
              "      <td>0.417983</td>\n",
              "      <td>0.362322</td>\n",
              "      <td>0.351995</td>\n",
              "      <td>0.391493</td>\n",
              "      <td>0.418305</td>\n",
              "      <td>0.440135</td>\n",
              "      <td>0.444598</td>\n",
              "      <td>0.460402</td>\n",
              "      <td>0.506810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.408587</td>\n",
              "      <td>0.401864</td>\n",
              "      <td>0.387069</td>\n",
              "      <td>0.359590</td>\n",
              "      <td>0.325879</td>\n",
              "      <td>0.288894</td>\n",
              "      <td>0.293521</td>\n",
              "      <td>0.344504</td>\n",
              "      <td>0.399012</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>0.682510</td>\n",
              "      <td>0.682286</td>\n",
              "      <td>0.641051</td>\n",
              "      <td>0.620212</td>\n",
              "      <td>0.608210</td>\n",
              "      <td>0.576331</td>\n",
              "      <td>0.603596</td>\n",
              "      <td>0.645714</td>\n",
              "      <td>0.677964</td>\n",
              "      <td>0.720297</td>\n",
              "      <td>...</td>\n",
              "      <td>0.452247</td>\n",
              "      <td>0.450421</td>\n",
              "      <td>0.439278</td>\n",
              "      <td>0.439086</td>\n",
              "      <td>0.394417</td>\n",
              "      <td>0.441650</td>\n",
              "      <td>0.473909</td>\n",
              "      <td>0.539199</td>\n",
              "      <td>0.547146</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>0.792175</td>\n",
              "      <td>0.815695</td>\n",
              "      <td>0.819518</td>\n",
              "      <td>0.820559</td>\n",
              "      <td>0.847985</td>\n",
              "      <td>0.880933</td>\n",
              "      <td>0.902061</td>\n",
              "      <td>0.878266</td>\n",
              "      <td>0.838806</td>\n",
              "      <td>0.811795</td>\n",
              "      <td>...</td>\n",
              "      <td>0.737351</td>\n",
              "      <td>0.778845</td>\n",
              "      <td>0.805446</td>\n",
              "      <td>0.782640</td>\n",
              "      <td>0.751236</td>\n",
              "      <td>0.741331</td>\n",
              "      <td>0.718790</td>\n",
              "      <td>0.714504</td>\n",
              "      <td>0.691004</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>928 rows × 256 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
              "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
              "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
              "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
              "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
              "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
              "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
              "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
              "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
              "\n",
              "            7         8         9  ...       246       247       248  \\\n",
              "0    0.750660  0.728282  0.707928  ...  0.637260  0.664539  0.667226   \n",
              "1    0.855127  0.835307  0.798640  ...  0.778790  0.806883  0.818640   \n",
              "2    0.286457  0.425022  0.611384  ...  0.000000  0.042690  0.165850   \n",
              "3    0.843187  0.846784  0.824438  ...  0.789156  0.793622  0.787665   \n",
              "4    0.446012  0.528910  0.634068  ...  0.200676  0.300147  0.407225   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "923  0.977826  0.956314  0.926773  ...  0.908312  0.926328  0.898749   \n",
              "924  0.821865  0.721302  0.612039  ...  0.429721  0.531567  0.642137   \n",
              "925  0.444598  0.460402  0.506810  ...  0.408587  0.401864  0.387069   \n",
              "926  0.645714  0.677964  0.720297  ...  0.452247  0.450421  0.439278   \n",
              "927  0.878266  0.838806  0.811795  ...  0.737351  0.778845  0.805446   \n",
              "\n",
              "          249       250       251       252       253       254  target  \n",
              "0    0.637064  0.593287  0.545503  0.515049  0.563257  0.633581       2  \n",
              "1    0.842472  0.866740  0.884152  0.897196  0.911293  0.922903       2  \n",
              "2    0.363445  0.549460  0.539346  0.522272  0.491668  0.454949       2  \n",
              "3    0.794515  0.796739  0.804063  0.809944  0.801814  0.777322       2  \n",
              "4    0.507346  0.605953  0.699309  0.790334  0.856593  0.849957       2  \n",
              "..        ...       ...       ...       ...       ...       ...     ...  \n",
              "923  0.855709  0.823132  0.815458  0.818083  0.829300  0.822382       3  \n",
              "924  0.742063  0.833042  0.814867  0.777622  0.760714  0.759294       3  \n",
              "925  0.359590  0.325879  0.288894  0.293521  0.344504  0.399012       3  \n",
              "926  0.439086  0.394417  0.441650  0.473909  0.539199  0.547146       3  \n",
              "927  0.782640  0.751236  0.741331  0.718790  0.714504  0.691004       3  \n",
              "\n",
              "[928 rows x 256 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#convert Target column values as Numeric using ngroups\n",
        "encode_target_label = df.groupby('Target').ngroup().rename(\"target\").to_frame()\n",
        "test_final  = df.merge(encode_target_label, left_index=True, right_index=True)\n",
        "test_final.drop(columns=['Target'],inplace=True)\n",
        "test_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRvRDvfrivtE"
      },
      "source": [
        "#### **PERFORM DIMENSIONALITY REDUCTION JUST FOR CHECKING/UNDERSTANDING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "scVPRQ3TZBaW",
        "outputId": "0b606399-2d4a-40a0-b9e3-cd7592305d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance of each component: [1.76145888e-01 9.50265614e-02 6.99060614e-02 6.15960001e-02\n",
            " 5.34876630e-02 4.23664893e-02 3.68320213e-02 3.38541791e-02\n",
            " 3.00884979e-02 2.90396728e-02 2.64962509e-02 2.42272738e-02\n",
            " 2.10221030e-02 1.99751559e-02 1.77321042e-02 1.63016802e-02\n",
            " 1.53898622e-02 1.48412074e-02 1.33644825e-02 1.19674074e-02\n",
            " 1.16813409e-02 1.05807650e-02 9.68875480e-03 9.47385060e-03\n",
            " 8.65347748e-03 8.47506998e-03 7.93382172e-03 7.30163338e-03\n",
            " 6.76380665e-03 6.36886390e-03 6.02004791e-03 5.46823032e-03\n",
            " 5.31229911e-03 4.97821789e-03 4.74686092e-03 4.46081684e-03\n",
            " 4.21254684e-03 4.01200243e-03 3.87246476e-03 3.52519084e-03\n",
            " 3.37596894e-03 3.26978336e-03 3.08241145e-03 2.96423495e-03\n",
            " 2.73419816e-03 2.50965698e-03 2.35335480e-03 2.25665349e-03\n",
            " 2.20141761e-03 1.96782025e-03 1.74343954e-03 1.70982830e-03\n",
            " 1.57456047e-03 1.53704487e-03 1.36768435e-03 1.33167096e-03\n",
            " 1.26444173e-03 1.20053330e-03 1.18738749e-03 1.08864087e-03\n",
            " 1.02824532e-03 9.11484783e-04 7.89962329e-04 7.59785111e-04\n",
            " 6.49920864e-04 6.27833793e-04 6.04784065e-04 5.45886709e-04\n",
            " 5.32310102e-04 4.97728350e-04 4.78393195e-04 4.50404967e-04\n",
            " 4.26173472e-04 4.09392370e-04 3.92601465e-04 3.70241593e-04\n",
            " 3.66854832e-04 3.38846611e-04 3.08205849e-04 3.00634223e-04\n",
            " 2.80363431e-04 2.67850883e-04 2.49661129e-04 2.40638587e-04\n",
            " 2.11564004e-04 2.03936838e-04 1.99023724e-04 1.83141332e-04\n",
            " 1.66818168e-04 1.65879817e-04 1.55966608e-04 1.39159748e-04\n",
            " 1.25331097e-04 1.22350036e-04 1.13547511e-04 1.09410388e-04\n",
            " 1.03461841e-04 9.93331689e-05 9.85714275e-05 9.43656877e-05]\n",
            "\n",
            " Total Variance Explained: 99.8\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.018578</td>\n",
              "      <td>1.148263</td>\n",
              "      <td>-0.589582</td>\n",
              "      <td>0.193617</td>\n",
              "      <td>0.047950</td>\n",
              "      <td>-0.309400</td>\n",
              "      <td>-0.161566</td>\n",
              "      <td>0.478471</td>\n",
              "      <td>0.972403</td>\n",
              "      <td>-0.031832</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012538</td>\n",
              "      <td>-0.003420</td>\n",
              "      <td>0.024055</td>\n",
              "      <td>0.014359</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.003836</td>\n",
              "      <td>0.004676</td>\n",
              "      <td>-0.063894</td>\n",
              "      <td>-0.023271</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.098692</td>\n",
              "      <td>0.289832</td>\n",
              "      <td>-1.766388</td>\n",
              "      <td>1.076165</td>\n",
              "      <td>-0.261201</td>\n",
              "      <td>-0.820446</td>\n",
              "      <td>-0.474188</td>\n",
              "      <td>-0.515238</td>\n",
              "      <td>0.692389</td>\n",
              "      <td>1.501606</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017249</td>\n",
              "      <td>0.060992</td>\n",
              "      <td>-0.006950</td>\n",
              "      <td>0.011650</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.045215</td>\n",
              "      <td>0.032347</td>\n",
              "      <td>-0.035253</td>\n",
              "      <td>0.003367</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.275021</td>\n",
              "      <td>-0.451289</td>\n",
              "      <td>0.106750</td>\n",
              "      <td>-0.426415</td>\n",
              "      <td>0.066133</td>\n",
              "      <td>0.692474</td>\n",
              "      <td>0.634894</td>\n",
              "      <td>-0.035867</td>\n",
              "      <td>0.815855</td>\n",
              "      <td>-0.909473</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044908</td>\n",
              "      <td>-0.041036</td>\n",
              "      <td>-0.029266</td>\n",
              "      <td>0.122238</td>\n",
              "      <td>-0.049328</td>\n",
              "      <td>-0.019353</td>\n",
              "      <td>-0.018893</td>\n",
              "      <td>0.048714</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.517085</td>\n",
              "      <td>1.662693</td>\n",
              "      <td>-1.021167</td>\n",
              "      <td>0.804267</td>\n",
              "      <td>-0.281985</td>\n",
              "      <td>0.518180</td>\n",
              "      <td>0.355748</td>\n",
              "      <td>-0.344235</td>\n",
              "      <td>-0.910867</td>\n",
              "      <td>-0.629517</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.040973</td>\n",
              "      <td>0.017538</td>\n",
              "      <td>-0.052479</td>\n",
              "      <td>-0.040780</td>\n",
              "      <td>-0.015963</td>\n",
              "      <td>-0.020239</td>\n",
              "      <td>0.023038</td>\n",
              "      <td>-0.038569</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.152840</td>\n",
              "      <td>-1.046283</td>\n",
              "      <td>0.351278</td>\n",
              "      <td>1.100381</td>\n",
              "      <td>-1.613642</td>\n",
              "      <td>1.484188</td>\n",
              "      <td>-0.113277</td>\n",
              "      <td>-0.251152</td>\n",
              "      <td>0.179023</td>\n",
              "      <td>-0.233104</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008958</td>\n",
              "      <td>0.055771</td>\n",
              "      <td>0.002886</td>\n",
              "      <td>0.030411</td>\n",
              "      <td>0.018307</td>\n",
              "      <td>-0.006205</td>\n",
              "      <td>0.041270</td>\n",
              "      <td>0.002578</td>\n",
              "      <td>-0.025562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>-1.321884</td>\n",
              "      <td>2.153021</td>\n",
              "      <td>0.788596</td>\n",
              "      <td>-1.304253</td>\n",
              "      <td>0.458186</td>\n",
              "      <td>-0.859346</td>\n",
              "      <td>-0.069127</td>\n",
              "      <td>-0.392796</td>\n",
              "      <td>0.755732</td>\n",
              "      <td>-0.584050</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037798</td>\n",
              "      <td>0.079481</td>\n",
              "      <td>-0.043129</td>\n",
              "      <td>-0.028721</td>\n",
              "      <td>0.007277</td>\n",
              "      <td>-0.024090</td>\n",
              "      <td>-0.049640</td>\n",
              "      <td>-0.003669</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>-0.867163</td>\n",
              "      <td>-0.040504</td>\n",
              "      <td>0.940680</td>\n",
              "      <td>0.302648</td>\n",
              "      <td>-0.469672</td>\n",
              "      <td>-0.368255</td>\n",
              "      <td>1.065579</td>\n",
              "      <td>0.801522</td>\n",
              "      <td>0.690113</td>\n",
              "      <td>0.953008</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013012</td>\n",
              "      <td>-0.092781</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>-0.029570</td>\n",
              "      <td>-0.008620</td>\n",
              "      <td>0.058530</td>\n",
              "      <td>-0.008667</td>\n",
              "      <td>-0.065701</td>\n",
              "      <td>-0.019623</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>3.753012</td>\n",
              "      <td>0.841636</td>\n",
              "      <td>-0.317393</td>\n",
              "      <td>-0.296117</td>\n",
              "      <td>0.593769</td>\n",
              "      <td>-0.255474</td>\n",
              "      <td>-0.057091</td>\n",
              "      <td>-0.072048</td>\n",
              "      <td>0.664386</td>\n",
              "      <td>-0.837668</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007735</td>\n",
              "      <td>-0.006026</td>\n",
              "      <td>-0.004219</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>-0.022751</td>\n",
              "      <td>0.068005</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>-0.006318</td>\n",
              "      <td>-0.007285</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>0.603083</td>\n",
              "      <td>0.126259</td>\n",
              "      <td>0.003433</td>\n",
              "      <td>0.283612</td>\n",
              "      <td>0.169559</td>\n",
              "      <td>-0.156326</td>\n",
              "      <td>-0.068399</td>\n",
              "      <td>-0.184308</td>\n",
              "      <td>0.461063</td>\n",
              "      <td>-0.002047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054908</td>\n",
              "      <td>0.024523</td>\n",
              "      <td>-0.014663</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.005506</td>\n",
              "      <td>0.035529</td>\n",
              "      <td>-0.053516</td>\n",
              "      <td>0.027977</td>\n",
              "      <td>0.059146</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>-1.452945</td>\n",
              "      <td>1.233599</td>\n",
              "      <td>0.439472</td>\n",
              "      <td>0.278517</td>\n",
              "      <td>0.165928</td>\n",
              "      <td>-0.171830</td>\n",
              "      <td>-0.075000</td>\n",
              "      <td>0.033859</td>\n",
              "      <td>0.004444</td>\n",
              "      <td>-0.687583</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012556</td>\n",
              "      <td>-0.030230</td>\n",
              "      <td>0.071278</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>0.016394</td>\n",
              "      <td>-0.074669</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>-0.049471</td>\n",
              "      <td>-0.025437</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>928 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
              "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
              "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
              "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
              "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
              "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
              "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
              "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
              "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
              "\n",
              "            7         8         9  ...        91        92        93  \\\n",
              "0    0.478471  0.972403 -0.031832  ...  0.012538 -0.003420  0.024055   \n",
              "1   -0.515238  0.692389  1.501606  ... -0.017249  0.060992 -0.006950   \n",
              "2   -0.035867  0.815855 -0.909473  ...  0.044908 -0.041036 -0.029266   \n",
              "3   -0.344235 -0.910867 -0.629517  ...  0.000652  0.040973  0.017538   \n",
              "4   -0.251152  0.179023 -0.233104  ... -0.008958  0.055771  0.002886   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "923 -0.392796  0.755732 -0.584050  ... -0.037798  0.079481 -0.043129   \n",
              "924  0.801522  0.690113  0.953008  ...  0.013012 -0.092781  0.013536   \n",
              "925 -0.072048  0.664386 -0.837668  ... -0.007735 -0.006026 -0.004219   \n",
              "926 -0.184308  0.461063 -0.002047  ...  0.054908  0.024523 -0.014663   \n",
              "927  0.033859  0.004444 -0.687583  ...  0.012556 -0.030230  0.071278   \n",
              "\n",
              "           94        95        96        97        98        99  target  \n",
              "0    0.014359  0.000052  0.003836  0.004676 -0.063894 -0.023271       2  \n",
              "1    0.011650  0.001496  0.045215  0.032347 -0.035253  0.003367       2  \n",
              "2    0.122238 -0.049328 -0.019353 -0.018893  0.048714  0.000011       2  \n",
              "3   -0.052479 -0.040780 -0.015963 -0.020239  0.023038 -0.038569       2  \n",
              "4    0.030411  0.018307 -0.006205  0.041270  0.002578 -0.025562       2  \n",
              "..        ...       ...       ...       ...       ...       ...     ...  \n",
              "923 -0.028721  0.007277 -0.024090 -0.049640 -0.003669  0.008759       3  \n",
              "924 -0.029570 -0.008620  0.058530 -0.008667 -0.065701 -0.019623       3  \n",
              "925  0.004492 -0.022751  0.068005  0.019306 -0.006318 -0.007285       3  \n",
              "926 -0.076395 -0.005506  0.035529 -0.053516  0.027977  0.059146       3  \n",
              "927 -0.062978  0.016394 -0.074669  0.001815 -0.049471 -0.025437       3  \n",
              "\n",
              "[928 rows x 101 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#just for testing\n",
        "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#do PCA and choose componeents as 100\n",
        "pca = PCA(n_components=100)\n",
        "x_pca = pca.fit_transform(test_final.iloc[:,0:-1])\n",
        "x_pca = pd.DataFrame(x_pca)\n",
        "\n",
        "# Calculate the variance explained by priciple components\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print('Variance of each component:', pca.explained_variance_ratio_)\n",
        "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
        "\n",
        "#store the new pca generated dimensions in a dataframe\n",
        "pca_df = pd.DataFrame(data = x_pca)\n",
        "target = pd.Series(test_final['target'], name='target')\n",
        "result_df = pd.concat([pca_df, target], axis=1)\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "ErMVKLMrKBiJ",
        "outputId": "f7ccdbdd-773b-4a0e-d7de-d467d120a211"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.018578</td>\n",
              "      <td>1.148263</td>\n",
              "      <td>-0.589582</td>\n",
              "      <td>0.193617</td>\n",
              "      <td>0.047950</td>\n",
              "      <td>-0.309400</td>\n",
              "      <td>-0.161566</td>\n",
              "      <td>0.478471</td>\n",
              "      <td>0.972403</td>\n",
              "      <td>-0.031832</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012538</td>\n",
              "      <td>-0.003420</td>\n",
              "      <td>0.024055</td>\n",
              "      <td>0.014359</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.003836</td>\n",
              "      <td>0.004676</td>\n",
              "      <td>-0.063894</td>\n",
              "      <td>-0.023271</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.098692</td>\n",
              "      <td>0.289832</td>\n",
              "      <td>-1.766388</td>\n",
              "      <td>1.076165</td>\n",
              "      <td>-0.261201</td>\n",
              "      <td>-0.820446</td>\n",
              "      <td>-0.474188</td>\n",
              "      <td>-0.515238</td>\n",
              "      <td>0.692389</td>\n",
              "      <td>1.501606</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017249</td>\n",
              "      <td>0.060992</td>\n",
              "      <td>-0.006950</td>\n",
              "      <td>0.011650</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>0.045215</td>\n",
              "      <td>0.032347</td>\n",
              "      <td>-0.035253</td>\n",
              "      <td>0.003367</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.275021</td>\n",
              "      <td>-0.451289</td>\n",
              "      <td>0.106750</td>\n",
              "      <td>-0.426415</td>\n",
              "      <td>0.066133</td>\n",
              "      <td>0.692474</td>\n",
              "      <td>0.634894</td>\n",
              "      <td>-0.035867</td>\n",
              "      <td>0.815855</td>\n",
              "      <td>-0.909473</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044908</td>\n",
              "      <td>-0.041036</td>\n",
              "      <td>-0.029266</td>\n",
              "      <td>0.122238</td>\n",
              "      <td>-0.049328</td>\n",
              "      <td>-0.019353</td>\n",
              "      <td>-0.018893</td>\n",
              "      <td>0.048714</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.517085</td>\n",
              "      <td>1.662693</td>\n",
              "      <td>-1.021167</td>\n",
              "      <td>0.804267</td>\n",
              "      <td>-0.281985</td>\n",
              "      <td>0.518180</td>\n",
              "      <td>0.355748</td>\n",
              "      <td>-0.344235</td>\n",
              "      <td>-0.910867</td>\n",
              "      <td>-0.629517</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.040973</td>\n",
              "      <td>0.017538</td>\n",
              "      <td>-0.052479</td>\n",
              "      <td>-0.040780</td>\n",
              "      <td>-0.015963</td>\n",
              "      <td>-0.020239</td>\n",
              "      <td>0.023038</td>\n",
              "      <td>-0.038569</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.152840</td>\n",
              "      <td>-1.046283</td>\n",
              "      <td>0.351278</td>\n",
              "      <td>1.100381</td>\n",
              "      <td>-1.613642</td>\n",
              "      <td>1.484188</td>\n",
              "      <td>-0.113277</td>\n",
              "      <td>-0.251152</td>\n",
              "      <td>0.179023</td>\n",
              "      <td>-0.233104</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008958</td>\n",
              "      <td>0.055771</td>\n",
              "      <td>0.002886</td>\n",
              "      <td>0.030411</td>\n",
              "      <td>0.018307</td>\n",
              "      <td>-0.006205</td>\n",
              "      <td>0.041270</td>\n",
              "      <td>0.002578</td>\n",
              "      <td>-0.025562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>-1.321884</td>\n",
              "      <td>2.153021</td>\n",
              "      <td>0.788596</td>\n",
              "      <td>-1.304253</td>\n",
              "      <td>0.458186</td>\n",
              "      <td>-0.859346</td>\n",
              "      <td>-0.069127</td>\n",
              "      <td>-0.392796</td>\n",
              "      <td>0.755732</td>\n",
              "      <td>-0.584050</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037798</td>\n",
              "      <td>0.079481</td>\n",
              "      <td>-0.043129</td>\n",
              "      <td>-0.028721</td>\n",
              "      <td>0.007277</td>\n",
              "      <td>-0.024090</td>\n",
              "      <td>-0.049640</td>\n",
              "      <td>-0.003669</td>\n",
              "      <td>0.008759</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>-0.867163</td>\n",
              "      <td>-0.040504</td>\n",
              "      <td>0.940680</td>\n",
              "      <td>0.302648</td>\n",
              "      <td>-0.469672</td>\n",
              "      <td>-0.368255</td>\n",
              "      <td>1.065579</td>\n",
              "      <td>0.801522</td>\n",
              "      <td>0.690113</td>\n",
              "      <td>0.953008</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013012</td>\n",
              "      <td>-0.092781</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>-0.029570</td>\n",
              "      <td>-0.008620</td>\n",
              "      <td>0.058530</td>\n",
              "      <td>-0.008667</td>\n",
              "      <td>-0.065701</td>\n",
              "      <td>-0.019623</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>3.753012</td>\n",
              "      <td>0.841636</td>\n",
              "      <td>-0.317393</td>\n",
              "      <td>-0.296117</td>\n",
              "      <td>0.593769</td>\n",
              "      <td>-0.255474</td>\n",
              "      <td>-0.057091</td>\n",
              "      <td>-0.072048</td>\n",
              "      <td>0.664386</td>\n",
              "      <td>-0.837668</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007735</td>\n",
              "      <td>-0.006026</td>\n",
              "      <td>-0.004219</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>-0.022751</td>\n",
              "      <td>0.068005</td>\n",
              "      <td>0.019306</td>\n",
              "      <td>-0.006318</td>\n",
              "      <td>-0.007285</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>0.603083</td>\n",
              "      <td>0.126259</td>\n",
              "      <td>0.003433</td>\n",
              "      <td>0.283612</td>\n",
              "      <td>0.169559</td>\n",
              "      <td>-0.156326</td>\n",
              "      <td>-0.068399</td>\n",
              "      <td>-0.184308</td>\n",
              "      <td>0.461063</td>\n",
              "      <td>-0.002047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054908</td>\n",
              "      <td>0.024523</td>\n",
              "      <td>-0.014663</td>\n",
              "      <td>-0.076395</td>\n",
              "      <td>-0.005506</td>\n",
              "      <td>0.035529</td>\n",
              "      <td>-0.053516</td>\n",
              "      <td>0.027977</td>\n",
              "      <td>0.059146</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>-1.452945</td>\n",
              "      <td>1.233599</td>\n",
              "      <td>0.439472</td>\n",
              "      <td>0.278517</td>\n",
              "      <td>0.165928</td>\n",
              "      <td>-0.171830</td>\n",
              "      <td>-0.075000</td>\n",
              "      <td>0.033859</td>\n",
              "      <td>0.004444</td>\n",
              "      <td>-0.687583</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012556</td>\n",
              "      <td>-0.030230</td>\n",
              "      <td>0.071278</td>\n",
              "      <td>-0.062978</td>\n",
              "      <td>0.016394</td>\n",
              "      <td>-0.074669</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>-0.049471</td>\n",
              "      <td>-0.025437</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>928 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
              "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
              "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
              "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
              "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
              "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
              "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
              "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
              "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
              "\n",
              "            7         8         9  ...        91        92        93  \\\n",
              "0    0.478471  0.972403 -0.031832  ...  0.012538 -0.003420  0.024055   \n",
              "1   -0.515238  0.692389  1.501606  ... -0.017249  0.060992 -0.006950   \n",
              "2   -0.035867  0.815855 -0.909473  ...  0.044908 -0.041036 -0.029266   \n",
              "3   -0.344235 -0.910867 -0.629517  ...  0.000652  0.040973  0.017538   \n",
              "4   -0.251152  0.179023 -0.233104  ... -0.008958  0.055771  0.002886   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "923 -0.392796  0.755732 -0.584050  ... -0.037798  0.079481 -0.043129   \n",
              "924  0.801522  0.690113  0.953008  ...  0.013012 -0.092781  0.013536   \n",
              "925 -0.072048  0.664386 -0.837668  ... -0.007735 -0.006026 -0.004219   \n",
              "926 -0.184308  0.461063 -0.002047  ...  0.054908  0.024523 -0.014663   \n",
              "927  0.033859  0.004444 -0.687583  ...  0.012556 -0.030230  0.071278   \n",
              "\n",
              "           94        95        96        97        98        99  target  \n",
              "0    0.014359  0.000052  0.003836  0.004676 -0.063894 -0.023271       2  \n",
              "1    0.011650  0.001496  0.045215  0.032347 -0.035253  0.003367       2  \n",
              "2    0.122238 -0.049328 -0.019353 -0.018893  0.048714  0.000011       2  \n",
              "3   -0.052479 -0.040780 -0.015963 -0.020239  0.023038 -0.038569       2  \n",
              "4    0.030411  0.018307 -0.006205  0.041270  0.002578 -0.025562       2  \n",
              "..        ...       ...       ...       ...       ...       ...     ...  \n",
              "923 -0.028721  0.007277 -0.024090 -0.049640 -0.003669  0.008759       3  \n",
              "924 -0.029570 -0.008620  0.058530 -0.008667 -0.065701 -0.019623       3  \n",
              "925  0.004492 -0.022751  0.068005  0.019306 -0.006318 -0.007285       3  \n",
              "926 -0.076395 -0.005506  0.035529 -0.053516  0.027977  0.059146       3  \n",
              "927 -0.062978  0.016394 -0.074669  0.001815 -0.049471 -0.025437       3  \n",
              "\n",
              "[928 rows x 101 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UOI2lH6iOIY"
      },
      "source": [
        "#### **TRYING DIFFERENT ML MODELS ON A SINGLE LEAD(EX : 1) POST DIMENSIONALITY REDUCTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE3l8LNQizo0"
      },
      "source": [
        "##### **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me4jqP4Zih_I",
        "outputId": "83dde896-7166-44cb-b88f-725eaedb9d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.782258064516129\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.63      0.73       105\n",
            "           1       0.91      0.91      0.91        94\n",
            "           2       0.72      0.88      0.79       112\n",
            "           3       0.63      0.67      0.65        61\n",
            "\n",
            "    accuracy                           0.78       372\n",
            "   macro avg       0.78      0.77      0.77       372\n",
            "weighted avg       0.80      0.78      0.78       372\n",
            "\n",
            "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary modules for ML model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup the pipeline steps:\n",
        "steps = [('knn', KNeighborsClassifier())]\n",
        "        \n",
        "# Create the pipeline: pipeline \n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
        "k_range = list(range(1, 9))\n",
        "parameters = dict(knn__n_neighbors=k_range)\n",
        "\n",
        "#input\n",
        "X = result_df.iloc[:,0:-1]\n",
        "\n",
        "#target\n",
        "y=result_df.iloc[:,-1]\n",
        "\n",
        "# Create train and test sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
        "\n",
        "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
        "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
        "\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "Knn_Accuracy = cv.score(X_test, y_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDkFlXChia-8"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKtMbgXLifbX"
      },
      "source": [
        "##### **LOGISTIC REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BbGt_eOVkJq6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup the pipeline steps:\n",
        "steps = [('lr', LogisticRegression())]\n",
        "        \n",
        "# Create the pipeline: pipeline \n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "#input\n",
        "X = result_df.iloc[:,0:-1]\n",
        "\n",
        "#target\n",
        "y=result_df.iloc[:,-1]\n",
        "\n",
        "#parameters for gridsearchcv if we increase range of entries from 5 to higher value, we can get greater accurange\n",
        "c_space = np.logspace(-4, 4, 3)\n",
        "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
        "\n",
        "#call GridSearchCV and set crossvalscore to 2\n",
        "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
        "\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred\n",
        "y_pred = cv.predict(X_test)\n",
        "LR_Accuracy = cv.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k9i12eGunX1",
        "outputId": "21ce6e7f-653e-4710-d585-ff445736e08b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5483870967741935\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.34      0.36       105\n",
            "           1       0.73      0.91      0.81        94\n",
            "           2       0.57      0.58      0.57       112\n",
            "           3       0.40      0.28      0.33        61\n",
            "\n",
            "    accuracy                           0.55       372\n",
            "   macro avg       0.52      0.53      0.52       372\n",
            "weighted avg       0.53      0.55      0.53       372\n",
            "\n",
            "Tuned Model Parameters: {'lr__C': 10000.0, 'lr__penalty': 'l2'}\n"
          ]
        }
      ],
      "source": [
        "# Compute and print metrics\n",
        "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W0sQQDwiisE"
      },
      "source": [
        "##### **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94xczE1iurC9",
        "outputId": "d5530fa8-865f-4ec0-80c9-f89940866c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8225806451612904\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      1.00      0.74        93\n",
            "           1       1.00      1.00      1.00        99\n",
            "           2       1.00      0.61      0.76       117\n",
            "           3       1.00      0.68      0.81        63\n",
            "\n",
            "    accuracy                           0.82       372\n",
            "   macro avg       0.90      0.82      0.83       372\n",
            "weighted avg       0.90      0.82      0.83       372\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary modules for ML model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup the pipeline\n",
        "steps = [('SVM', SVC())]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "#input\n",
        "X = result_df.iloc[:,0:-1]\n",
        "\n",
        "#target\n",
        "y=result_df.iloc[:,-1]\n",
        "\n",
        "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
        "#since it takes lots of time in google colab provided only a single value\n",
        "parameters = {'SVM__C':[10],'SVM__gamma':[1]}\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=21)\n",
        "\n",
        "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "y_pred = cv.predict(X_test)\n",
        "SVM_Accuracy = cv.score(X_test, y_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "SVM_Accuracy=cv.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1GNZtOUi6PP"
      },
      "source": [
        "#### **NOW COMBINING ALL 12 LEADS INTO A SINGLE CSV FILE AND THEN PERFROM MODEL ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "huNy0hsWSkr5",
        "outputId": "ae4fc5c3-8b46-4d0a-d075-c2c64559680c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>3051</th>\n",
              "      <th>3052</th>\n",
              "      <th>3053</th>\n",
              "      <th>3054</th>\n",
              "      <th>3055</th>\n",
              "      <th>3056</th>\n",
              "      <th>3057</th>\n",
              "      <th>3058</th>\n",
              "      <th>3059</th>\n",
              "      <th>3060</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.728449</td>\n",
              "      <td>0.680755</td>\n",
              "      <td>0.619010</td>\n",
              "      <td>0.645367</td>\n",
              "      <td>0.681570</td>\n",
              "      <td>0.732488</td>\n",
              "      <td>0.758448</td>\n",
              "      <td>0.750660</td>\n",
              "      <td>0.728282</td>\n",
              "      <td>0.707928</td>\n",
              "      <td>...</td>\n",
              "      <td>0.864067</td>\n",
              "      <td>0.849256</td>\n",
              "      <td>0.854949</td>\n",
              "      <td>0.861380</td>\n",
              "      <td>0.875514</td>\n",
              "      <td>0.868763</td>\n",
              "      <td>0.847450</td>\n",
              "      <td>0.805689</td>\n",
              "      <td>0.751761</td>\n",
              "      <td>0.702102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.957972</td>\n",
              "      <td>0.950695</td>\n",
              "      <td>0.941024</td>\n",
              "      <td>0.930501</td>\n",
              "      <td>0.913601</td>\n",
              "      <td>0.892244</td>\n",
              "      <td>0.868016</td>\n",
              "      <td>0.855127</td>\n",
              "      <td>0.835307</td>\n",
              "      <td>0.798640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.925865</td>\n",
              "      <td>0.928285</td>\n",
              "      <td>0.946033</td>\n",
              "      <td>0.947274</td>\n",
              "      <td>0.946394</td>\n",
              "      <td>0.936536</td>\n",
              "      <td>0.920869</td>\n",
              "      <td>0.910320</td>\n",
              "      <td>0.905436</td>\n",
              "      <td>0.876942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.611084</td>\n",
              "      <td>0.661575</td>\n",
              "      <td>0.695790</td>\n",
              "      <td>0.741113</td>\n",
              "      <td>0.716666</td>\n",
              "      <td>0.595794</td>\n",
              "      <td>0.425022</td>\n",
              "      <td>0.286457</td>\n",
              "      <td>0.425022</td>\n",
              "      <td>0.611384</td>\n",
              "      <td>...</td>\n",
              "      <td>0.170137</td>\n",
              "      <td>0.166206</td>\n",
              "      <td>0.207633</td>\n",
              "      <td>0.258184</td>\n",
              "      <td>0.286993</td>\n",
              "      <td>0.304742</td>\n",
              "      <td>0.325659</td>\n",
              "      <td>0.361189</td>\n",
              "      <td>0.451946</td>\n",
              "      <td>0.543373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.839213</td>\n",
              "      <td>0.861690</td>\n",
              "      <td>0.866457</td>\n",
              "      <td>0.865756</td>\n",
              "      <td>0.855027</td>\n",
              "      <td>0.855606</td>\n",
              "      <td>0.845561</td>\n",
              "      <td>0.843187</td>\n",
              "      <td>0.846784</td>\n",
              "      <td>0.824438</td>\n",
              "      <td>...</td>\n",
              "      <td>0.880043</td>\n",
              "      <td>0.883833</td>\n",
              "      <td>0.870995</td>\n",
              "      <td>0.861323</td>\n",
              "      <td>0.864892</td>\n",
              "      <td>0.863552</td>\n",
              "      <td>0.839506</td>\n",
              "      <td>0.805486</td>\n",
              "      <td>0.801828</td>\n",
              "      <td>0.826618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.917753</td>\n",
              "      <td>0.924369</td>\n",
              "      <td>0.873765</td>\n",
              "      <td>0.791381</td>\n",
              "      <td>0.699513</td>\n",
              "      <td>0.604927</td>\n",
              "      <td>0.500312</td>\n",
              "      <td>0.446012</td>\n",
              "      <td>0.528910</td>\n",
              "      <td>0.634068</td>\n",
              "      <td>...</td>\n",
              "      <td>0.400774</td>\n",
              "      <td>0.380920</td>\n",
              "      <td>0.439510</td>\n",
              "      <td>0.505257</td>\n",
              "      <td>0.561538</td>\n",
              "      <td>0.577997</td>\n",
              "      <td>0.566082</td>\n",
              "      <td>0.547642</td>\n",
              "      <td>0.538735</td>\n",
              "      <td>0.527560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>0.874246</td>\n",
              "      <td>0.877014</td>\n",
              "      <td>0.864280</td>\n",
              "      <td>0.860505</td>\n",
              "      <td>0.871349</td>\n",
              "      <td>0.912404</td>\n",
              "      <td>0.958148</td>\n",
              "      <td>0.977826</td>\n",
              "      <td>0.956314</td>\n",
              "      <td>0.926773</td>\n",
              "      <td>...</td>\n",
              "      <td>0.785103</td>\n",
              "      <td>0.676795</td>\n",
              "      <td>0.579054</td>\n",
              "      <td>0.476613</td>\n",
              "      <td>0.458748</td>\n",
              "      <td>0.565470</td>\n",
              "      <td>0.681896</td>\n",
              "      <td>0.792646</td>\n",
              "      <td>0.871660</td>\n",
              "      <td>0.872789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>0.829815</td>\n",
              "      <td>0.832084</td>\n",
              "      <td>0.852396</td>\n",
              "      <td>0.909665</td>\n",
              "      <td>0.988242</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.923323</td>\n",
              "      <td>0.821865</td>\n",
              "      <td>0.721302</td>\n",
              "      <td>0.612039</td>\n",
              "      <td>...</td>\n",
              "      <td>0.913404</td>\n",
              "      <td>0.968015</td>\n",
              "      <td>0.992614</td>\n",
              "      <td>0.945789</td>\n",
              "      <td>0.876660</td>\n",
              "      <td>0.808906</td>\n",
              "      <td>0.741645</td>\n",
              "      <td>0.736615</td>\n",
              "      <td>0.797729</td>\n",
              "      <td>0.855637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>0.469048</td>\n",
              "      <td>0.417983</td>\n",
              "      <td>0.362322</td>\n",
              "      <td>0.351995</td>\n",
              "      <td>0.391493</td>\n",
              "      <td>0.418305</td>\n",
              "      <td>0.440135</td>\n",
              "      <td>0.444598</td>\n",
              "      <td>0.460402</td>\n",
              "      <td>0.506810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.730354</td>\n",
              "      <td>0.697465</td>\n",
              "      <td>0.714527</td>\n",
              "      <td>0.745605</td>\n",
              "      <td>0.754952</td>\n",
              "      <td>0.755059</td>\n",
              "      <td>0.755059</td>\n",
              "      <td>0.755093</td>\n",
              "      <td>0.759093</td>\n",
              "      <td>0.767555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>0.682510</td>\n",
              "      <td>0.682286</td>\n",
              "      <td>0.641051</td>\n",
              "      <td>0.620212</td>\n",
              "      <td>0.608210</td>\n",
              "      <td>0.576331</td>\n",
              "      <td>0.603596</td>\n",
              "      <td>0.645714</td>\n",
              "      <td>0.677964</td>\n",
              "      <td>0.720297</td>\n",
              "      <td>...</td>\n",
              "      <td>0.860939</td>\n",
              "      <td>0.824976</td>\n",
              "      <td>0.783459</td>\n",
              "      <td>0.761391</td>\n",
              "      <td>0.741917</td>\n",
              "      <td>0.770631</td>\n",
              "      <td>0.802701</td>\n",
              "      <td>0.821503</td>\n",
              "      <td>0.846300</td>\n",
              "      <td>0.858795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>0.792175</td>\n",
              "      <td>0.815695</td>\n",
              "      <td>0.819518</td>\n",
              "      <td>0.820559</td>\n",
              "      <td>0.847985</td>\n",
              "      <td>0.880933</td>\n",
              "      <td>0.902061</td>\n",
              "      <td>0.878266</td>\n",
              "      <td>0.838806</td>\n",
              "      <td>0.811795</td>\n",
              "      <td>...</td>\n",
              "      <td>0.516381</td>\n",
              "      <td>0.521133</td>\n",
              "      <td>0.521142</td>\n",
              "      <td>0.521143</td>\n",
              "      <td>0.522801</td>\n",
              "      <td>0.543166</td>\n",
              "      <td>0.549073</td>\n",
              "      <td>0.564977</td>\n",
              "      <td>0.576139</td>\n",
              "      <td>0.576267</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>928 rows × 3060 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6     \\\n",
              "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
              "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
              "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
              "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
              "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
              "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
              "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
              "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
              "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
              "\n",
              "         7         8         9     ...      3051      3052      3053  \\\n",
              "0    0.750660  0.728282  0.707928  ...  0.864067  0.849256  0.854949   \n",
              "1    0.855127  0.835307  0.798640  ...  0.925865  0.928285  0.946033   \n",
              "2    0.286457  0.425022  0.611384  ...  0.170137  0.166206  0.207633   \n",
              "3    0.843187  0.846784  0.824438  ...  0.880043  0.883833  0.870995   \n",
              "4    0.446012  0.528910  0.634068  ...  0.400774  0.380920  0.439510   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "923  0.977826  0.956314  0.926773  ...  0.785103  0.676795  0.579054   \n",
              "924  0.821865  0.721302  0.612039  ...  0.913404  0.968015  0.992614   \n",
              "925  0.444598  0.460402  0.506810  ...  0.730354  0.697465  0.714527   \n",
              "926  0.645714  0.677964  0.720297  ...  0.860939  0.824976  0.783459   \n",
              "927  0.878266  0.838806  0.811795  ...  0.516381  0.521133  0.521142   \n",
              "\n",
              "         3054      3055      3056      3057      3058      3059      3060  \n",
              "0    0.861380  0.875514  0.868763  0.847450  0.805689  0.751761  0.702102  \n",
              "1    0.947274  0.946394  0.936536  0.920869  0.910320  0.905436  0.876942  \n",
              "2    0.258184  0.286993  0.304742  0.325659  0.361189  0.451946  0.543373  \n",
              "3    0.861323  0.864892  0.863552  0.839506  0.805486  0.801828  0.826618  \n",
              "4    0.505257  0.561538  0.577997  0.566082  0.547642  0.538735  0.527560  \n",
              "..        ...       ...       ...       ...       ...       ...       ...  \n",
              "923  0.476613  0.458748  0.565470  0.681896  0.792646  0.871660  0.872789  \n",
              "924  0.945789  0.876660  0.808906  0.741645  0.736615  0.797729  0.855637  \n",
              "925  0.745605  0.754952  0.755059  0.755059  0.755093  0.759093  0.767555  \n",
              "926  0.761391  0.741917  0.770631  0.802701  0.821503  0.846300  0.858795  \n",
              "927  0.521143  0.522801  0.543166  0.549073  0.564977  0.576139  0.576267  \n",
              "\n",
              "[928 rows x 3060 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#lets try combining all 12 leads in a single csv\n",
        "location= 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\Combined1d_csv'\n",
        "for files in natsorted(os.listdir(location)):\n",
        "  if files.endswith(\".csv\") and not files.endswith(\"13.csv\"):\n",
        "    if files!='Combined_IDLead_1.csv':\n",
        "      df=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\Cardiovascular-Detection-using-ECG-images\\\\Combined1d_csv\\\\{}'.format(files))\n",
        "      df.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "      test_final=pd.concat([test_final,df],axis=1,ignore_index=True)\n",
        "      test_final.drop(columns=test_final.columns[-1],axis=1,inplace=True)\n",
        "\n",
        "#drop the target column\n",
        "test_final.drop(columns=[255],axis=1,inplace=True)\n",
        "test_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OZ6_Lg0180y3"
      },
      "outputs": [],
      "source": [
        "#write the final file to csv\n",
        "test_final.to_csv('final_1D.csv',header=False,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxWK-X-qjde2"
      },
      "source": [
        "#### **TEST DIMENSIONALITY REDUCTION EXPLAINED VARIANCE ON  THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WXvZGdh5cxrL",
        "outputId": "63b7ea81-03df-43ae-b708-630b9ce6722f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance of each component: [8.04649534e-02 4.68818003e-02 3.76212504e-02 2.94708618e-02\n",
            " 2.57031130e-02 2.32574514e-02 2.14376788e-02 2.04315151e-02\n",
            " 1.94482863e-02 1.79877408e-02 1.64766264e-02 1.53241665e-02\n",
            " 1.50689862e-02 1.41398267e-02 1.36330466e-02 1.33375324e-02\n",
            " 1.26355566e-02 1.25577001e-02 1.16968257e-02 1.11671338e-02\n",
            " 1.07975552e-02 1.06183806e-02 1.03402122e-02 1.01248410e-02\n",
            " 9.73197948e-03 9.25504395e-03 9.16367637e-03 8.76267060e-03\n",
            " 8.54270112e-03 8.20665462e-03 8.07642149e-03 7.90742343e-03\n",
            " 7.54929819e-03 7.21938018e-03 7.07604659e-03 6.89135251e-03\n",
            " 6.80575532e-03 6.71875790e-03 6.38252148e-03 6.33951897e-03\n",
            " 6.10254734e-03 5.94560955e-03 5.76371295e-03 5.71788829e-03\n",
            " 5.55354810e-03 5.42316932e-03 5.35640711e-03 5.08429353e-03\n",
            " 5.03302777e-03 4.96811576e-03 4.87696491e-03 4.63686128e-03\n",
            " 4.55349933e-03 4.45390625e-03 4.31579996e-03 4.28316592e-03\n",
            " 4.17213140e-03 4.12346241e-03 4.09072049e-03 3.99349122e-03\n",
            " 3.92129459e-03 3.81982060e-03 3.78116652e-03 3.73307150e-03\n",
            " 3.68894307e-03 3.55238746e-03 3.49148625e-03 3.40490507e-03\n",
            " 3.33593814e-03 3.25467389e-03 3.20023474e-03 3.14871964e-03\n",
            " 3.09091665e-03 3.07180393e-03 3.05651457e-03 2.95447952e-03\n",
            " 2.90507083e-03 2.84618700e-03 2.80939396e-03 2.76324718e-03\n",
            " 2.71487874e-03 2.68959207e-03 2.67378836e-03 2.62085254e-03\n",
            " 2.55991613e-03 2.53614502e-03 2.47015404e-03 2.45768102e-03\n",
            " 2.41851536e-03 2.39477316e-03 2.35560704e-03 2.29236345e-03\n",
            " 2.26928539e-03 2.24965527e-03 2.22764534e-03 2.19258829e-03\n",
            " 2.14654982e-03 2.09081474e-03 2.08656961e-03 2.04315332e-03\n",
            " 2.01191187e-03 1.99715030e-03 1.98092986e-03 1.93183566e-03\n",
            " 1.90133601e-03 1.86628808e-03 1.85847904e-03 1.79040117e-03\n",
            " 1.77318190e-03 1.76278440e-03 1.73682193e-03 1.70177712e-03\n",
            " 1.69142157e-03 1.66289246e-03 1.64192361e-03 1.62455779e-03\n",
            " 1.59836820e-03 1.57166872e-03 1.56017874e-03 1.55193712e-03\n",
            " 1.52130395e-03 1.50860404e-03 1.48563216e-03 1.45667689e-03\n",
            " 1.44862677e-03 1.43014707e-03 1.42443426e-03 1.39341888e-03\n",
            " 1.38941740e-03 1.38032166e-03 1.35292505e-03 1.33403513e-03\n",
            " 1.33300728e-03 1.31774024e-03 1.29238722e-03 1.24574072e-03\n",
            " 1.23408862e-03 1.21598644e-03 1.20568485e-03 1.19391143e-03\n",
            " 1.18690274e-03 1.16630751e-03 1.16159095e-03 1.14539199e-03\n",
            " 1.13634359e-03 1.11858663e-03 1.10460060e-03 1.08515359e-03\n",
            " 1.07679695e-03 1.06488284e-03 1.05861426e-03 1.04012565e-03\n",
            " 1.03222232e-03 1.02519590e-03 1.01169941e-03 9.96444257e-04\n",
            " 9.76134514e-04 9.61104386e-04 9.57134099e-04 9.48294848e-04\n",
            " 9.35386446e-04 9.29858628e-04 9.24107282e-04 9.20229599e-04\n",
            " 9.00136970e-04 8.84392791e-04 8.60041244e-04 8.58222437e-04\n",
            " 8.39586154e-04 8.34156616e-04 8.24745137e-04 8.19630377e-04\n",
            " 8.11755902e-04 8.09589697e-04 7.93351930e-04 7.83229226e-04\n",
            " 7.69323633e-04 7.62916710e-04 7.61217310e-04 7.49412461e-04\n",
            " 7.41978508e-04 7.32319449e-04 7.28386324e-04 7.15766463e-04\n",
            " 7.00416470e-04 6.92792928e-04 6.87860571e-04 6.77118996e-04\n",
            " 6.69195650e-04 6.62776506e-04 6.52787237e-04 6.41350808e-04\n",
            " 6.31671343e-04 6.25941688e-04 6.20986817e-04 6.12964320e-04\n",
            " 6.06757241e-04 6.00414979e-04 5.90442751e-04 5.85447566e-04\n",
            " 5.82053388e-04 5.72736727e-04 5.64768427e-04 5.62060875e-04\n",
            " 5.53942337e-04 5.47413377e-04 5.43815848e-04 5.39018247e-04\n",
            " 5.31538797e-04 5.21422265e-04 5.16620308e-04 5.13730678e-04\n",
            " 5.08883050e-04 5.04308686e-04 4.96238365e-04 4.91958415e-04\n",
            " 4.80055673e-04 4.74422583e-04 4.69414331e-04 4.65649131e-04\n",
            " 4.62052063e-04 4.58664175e-04 4.49131976e-04 4.46512859e-04\n",
            " 4.45747676e-04 4.36928353e-04 4.30056928e-04 4.24233410e-04\n",
            " 4.21656146e-04 4.20467963e-04 4.16760269e-04 4.15888837e-04\n",
            " 4.07286799e-04 4.03273137e-04 3.97207457e-04 3.91816956e-04\n",
            " 3.87932378e-04 3.83350757e-04 3.82143413e-04 3.79404864e-04\n",
            " 3.72582509e-04 3.63610217e-04 3.59554000e-04 3.56530331e-04\n",
            " 3.53235193e-04 3.50500791e-04 3.47735149e-04 3.42040356e-04\n",
            " 3.38310299e-04 3.37306466e-04 3.35511677e-04 3.28894991e-04\n",
            " 3.28218988e-04 3.24306970e-04 3.20401215e-04 3.14252985e-04\n",
            " 3.10208378e-04 3.06218856e-04 3.04335203e-04 3.01021089e-04\n",
            " 2.97566449e-04 2.95304849e-04 2.91254934e-04 2.89125011e-04\n",
            " 2.82693306e-04 2.76521039e-04 2.75268971e-04 2.74379630e-04\n",
            " 2.68266983e-04 2.68061586e-04 2.66099951e-04 2.64421178e-04\n",
            " 2.63833920e-04 2.57431376e-04 2.54138617e-04 2.51605600e-04\n",
            " 2.46905344e-04 2.43635650e-04 2.42082820e-04 2.40633674e-04\n",
            " 2.37784391e-04 2.37571666e-04 2.35490056e-04 2.31228645e-04\n",
            " 2.29666599e-04 2.22983452e-04 2.22296937e-04 2.19367166e-04\n",
            " 2.18709641e-04 2.16024373e-04 2.12842057e-04 2.10474280e-04\n",
            " 2.08847162e-04 2.05998247e-04 2.04966838e-04 2.02934881e-04\n",
            " 1.99807314e-04 1.96638456e-04 1.94996832e-04 1.93137042e-04\n",
            " 1.91195391e-04 1.89711576e-04 1.88308729e-04 1.84368169e-04\n",
            " 1.82566032e-04 1.80379240e-04 1.76609236e-04 1.75642365e-04\n",
            " 1.72429956e-04 1.71790337e-04 1.71184274e-04 1.69724720e-04\n",
            " 1.67453820e-04 1.65837422e-04 1.63727585e-04 1.59522627e-04\n",
            " 1.59156008e-04 1.57752362e-04 1.56407331e-04 1.54531908e-04\n",
            " 1.51818491e-04 1.51099381e-04 1.49353996e-04 1.45445141e-04\n",
            " 1.42820092e-04 1.41655962e-04 1.38456952e-04 1.37006592e-04\n",
            " 1.36526911e-04 1.35596730e-04 1.33874640e-04 1.32400596e-04\n",
            " 1.31171820e-04 1.30735287e-04 1.29583970e-04 1.25903811e-04\n",
            " 1.25102032e-04 1.23621872e-04 1.22234422e-04 1.21546010e-04\n",
            " 1.20740406e-04 1.18338920e-04 1.17245449e-04 1.15545848e-04\n",
            " 1.14239311e-04 1.13097211e-04 1.11419473e-04 1.10109794e-04\n",
            " 1.08572246e-04 1.08212342e-04 1.06961430e-04 1.04875966e-04\n",
            " 1.03980812e-04 1.02749763e-04 1.02001345e-04 1.00872954e-04\n",
            " 9.90641462e-05 9.81347698e-05 9.72738628e-05 9.51786834e-05\n",
            " 9.47705455e-05 9.39498856e-05 9.26728047e-05 9.07999723e-05\n",
            " 8.89750388e-05 8.79234081e-05 8.68213987e-05 8.57836182e-05\n",
            " 8.47756111e-05 8.38692150e-05 8.28270423e-05 8.15294472e-05\n",
            " 8.09802028e-05 8.03620942e-05 7.97282303e-05 7.84335025e-05\n",
            " 7.72564776e-05 7.69550751e-05 7.54850434e-05 7.47309363e-05\n",
            " 7.36208415e-05 7.30511576e-05 7.19647457e-05 7.04682594e-05\n",
            " 6.97877716e-05 6.91816281e-05 6.82912702e-05 6.69884949e-05\n",
            " 6.62261823e-05 6.39251889e-05 6.36094642e-05 6.27951178e-05\n",
            " 6.13682699e-05 6.10079772e-05 6.01931611e-05 5.91268622e-05\n",
            " 5.85853316e-05 5.81769451e-05 5.79516475e-05 5.60310913e-05]\n",
            "\n",
            " Total Variance Explained: 99.7\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>391</th>\n",
              "      <th>392</th>\n",
              "      <th>393</th>\n",
              "      <th>394</th>\n",
              "      <th>395</th>\n",
              "      <th>396</th>\n",
              "      <th>397</th>\n",
              "      <th>398</th>\n",
              "      <th>399</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.457260</td>\n",
              "      <td>0.433402</td>\n",
              "      <td>-0.951313</td>\n",
              "      <td>-0.118767</td>\n",
              "      <td>0.372034</td>\n",
              "      <td>1.917341</td>\n",
              "      <td>0.503248</td>\n",
              "      <td>-0.996278</td>\n",
              "      <td>-2.306744</td>\n",
              "      <td>-1.294890</td>\n",
              "      <td>...</td>\n",
              "      <td>0.065952</td>\n",
              "      <td>-0.009361</td>\n",
              "      <td>-0.079343</td>\n",
              "      <td>0.130464</td>\n",
              "      <td>-0.105183</td>\n",
              "      <td>0.076634</td>\n",
              "      <td>0.052387</td>\n",
              "      <td>0.122587</td>\n",
              "      <td>0.053437</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.482871</td>\n",
              "      <td>-3.923833</td>\n",
              "      <td>1.001672</td>\n",
              "      <td>1.151786</td>\n",
              "      <td>-0.105634</td>\n",
              "      <td>0.935446</td>\n",
              "      <td>0.061538</td>\n",
              "      <td>1.340459</td>\n",
              "      <td>-4.633610</td>\n",
              "      <td>-0.483502</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.123603</td>\n",
              "      <td>0.120566</td>\n",
              "      <td>0.191876</td>\n",
              "      <td>-0.062997</td>\n",
              "      <td>0.043188</td>\n",
              "      <td>0.164884</td>\n",
              "      <td>-0.023881</td>\n",
              "      <td>0.080277</td>\n",
              "      <td>0.083184</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.524849</td>\n",
              "      <td>-0.064380</td>\n",
              "      <td>-2.177830</td>\n",
              "      <td>-0.744632</td>\n",
              "      <td>-3.160053</td>\n",
              "      <td>-2.079289</td>\n",
              "      <td>1.991373</td>\n",
              "      <td>2.629458</td>\n",
              "      <td>1.251710</td>\n",
              "      <td>1.656291</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047435</td>\n",
              "      <td>-0.052997</td>\n",
              "      <td>-0.002885</td>\n",
              "      <td>0.076438</td>\n",
              "      <td>0.031182</td>\n",
              "      <td>-0.083977</td>\n",
              "      <td>0.022708</td>\n",
              "      <td>0.064800</td>\n",
              "      <td>-0.044042</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-4.626337</td>\n",
              "      <td>-0.901579</td>\n",
              "      <td>-1.351431</td>\n",
              "      <td>-1.767117</td>\n",
              "      <td>0.870166</td>\n",
              "      <td>2.210025</td>\n",
              "      <td>3.710694</td>\n",
              "      <td>-1.478115</td>\n",
              "      <td>-2.406238</td>\n",
              "      <td>-3.754295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144300</td>\n",
              "      <td>-0.015915</td>\n",
              "      <td>0.102075</td>\n",
              "      <td>-0.010077</td>\n",
              "      <td>-0.036840</td>\n",
              "      <td>-0.126834</td>\n",
              "      <td>0.034503</td>\n",
              "      <td>-0.127597</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-3.863027</td>\n",
              "      <td>-2.805259</td>\n",
              "      <td>-0.428457</td>\n",
              "      <td>-0.482452</td>\n",
              "      <td>0.231498</td>\n",
              "      <td>1.977056</td>\n",
              "      <td>1.605510</td>\n",
              "      <td>-0.603866</td>\n",
              "      <td>-1.312170</td>\n",
              "      <td>3.553396</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036211</td>\n",
              "      <td>-0.050934</td>\n",
              "      <td>0.042479</td>\n",
              "      <td>0.026889</td>\n",
              "      <td>0.060875</td>\n",
              "      <td>-0.038849</td>\n",
              "      <td>0.132917</td>\n",
              "      <td>0.214894</td>\n",
              "      <td>-0.152744</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>-3.365774</td>\n",
              "      <td>-2.530997</td>\n",
              "      <td>-0.781861</td>\n",
              "      <td>0.567793</td>\n",
              "      <td>1.960806</td>\n",
              "      <td>2.319130</td>\n",
              "      <td>3.632739</td>\n",
              "      <td>2.072028</td>\n",
              "      <td>0.318600</td>\n",
              "      <td>1.423024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006278</td>\n",
              "      <td>0.026028</td>\n",
              "      <td>0.054533</td>\n",
              "      <td>-0.212951</td>\n",
              "      <td>-0.135672</td>\n",
              "      <td>0.091030</td>\n",
              "      <td>-0.170808</td>\n",
              "      <td>-0.020764</td>\n",
              "      <td>-0.096268</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>-6.340754</td>\n",
              "      <td>0.634164</td>\n",
              "      <td>-3.259888</td>\n",
              "      <td>2.894988</td>\n",
              "      <td>2.625952</td>\n",
              "      <td>-1.435685</td>\n",
              "      <td>1.570297</td>\n",
              "      <td>0.456509</td>\n",
              "      <td>-0.855480</td>\n",
              "      <td>0.523852</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083129</td>\n",
              "      <td>0.289920</td>\n",
              "      <td>0.110156</td>\n",
              "      <td>0.001994</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>0.126620</td>\n",
              "      <td>-0.125365</td>\n",
              "      <td>-0.019861</td>\n",
              "      <td>0.134755</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>-2.106097</td>\n",
              "      <td>4.489429</td>\n",
              "      <td>4.740686</td>\n",
              "      <td>2.521037</td>\n",
              "      <td>1.810682</td>\n",
              "      <td>1.252919</td>\n",
              "      <td>-0.925497</td>\n",
              "      <td>2.413092</td>\n",
              "      <td>-0.426080</td>\n",
              "      <td>-2.335654</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028829</td>\n",
              "      <td>0.041517</td>\n",
              "      <td>0.031588</td>\n",
              "      <td>0.046523</td>\n",
              "      <td>0.078297</td>\n",
              "      <td>-0.203419</td>\n",
              "      <td>0.159186</td>\n",
              "      <td>0.044951</td>\n",
              "      <td>0.038521</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>-1.091978</td>\n",
              "      <td>7.063249</td>\n",
              "      <td>2.692234</td>\n",
              "      <td>-0.543705</td>\n",
              "      <td>-0.443376</td>\n",
              "      <td>1.028903</td>\n",
              "      <td>-1.749000</td>\n",
              "      <td>0.469497</td>\n",
              "      <td>-0.575796</td>\n",
              "      <td>-1.181286</td>\n",
              "      <td>...</td>\n",
              "      <td>0.052730</td>\n",
              "      <td>-0.093833</td>\n",
              "      <td>0.124342</td>\n",
              "      <td>-0.076364</td>\n",
              "      <td>0.001446</td>\n",
              "      <td>0.065485</td>\n",
              "      <td>-0.183273</td>\n",
              "      <td>0.038104</td>\n",
              "      <td>-0.233730</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>6.346374</td>\n",
              "      <td>-3.865858</td>\n",
              "      <td>6.209416</td>\n",
              "      <td>1.247192</td>\n",
              "      <td>0.112905</td>\n",
              "      <td>0.278156</td>\n",
              "      <td>-0.507665</td>\n",
              "      <td>-0.342197</td>\n",
              "      <td>-0.152731</td>\n",
              "      <td>-3.172417</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017154</td>\n",
              "      <td>0.061126</td>\n",
              "      <td>-0.064348</td>\n",
              "      <td>-0.031570</td>\n",
              "      <td>0.149201</td>\n",
              "      <td>0.126805</td>\n",
              "      <td>-0.180114</td>\n",
              "      <td>-0.127870</td>\n",
              "      <td>-0.013958</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>928 rows × 401 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "0   -0.457260  0.433402 -0.951313 -0.118767  0.372034  1.917341  0.503248   \n",
              "1   -1.482871 -3.923833  1.001672  1.151786 -0.105634  0.935446  0.061538   \n",
              "2    6.524849 -0.064380 -2.177830 -0.744632 -3.160053 -2.079289  1.991373   \n",
              "3   -4.626337 -0.901579 -1.351431 -1.767117  0.870166  2.210025  3.710694   \n",
              "4   -3.863027 -2.805259 -0.428457 -0.482452  0.231498  1.977056  1.605510   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "923 -3.365774 -2.530997 -0.781861  0.567793  1.960806  2.319130  3.632739   \n",
              "924 -6.340754  0.634164 -3.259888  2.894988  2.625952 -1.435685  1.570297   \n",
              "925 -2.106097  4.489429  4.740686  2.521037  1.810682  1.252919 -0.925497   \n",
              "926 -1.091978  7.063249  2.692234 -0.543705 -0.443376  1.028903 -1.749000   \n",
              "927  6.346374 -3.865858  6.209416  1.247192  0.112905  0.278156 -0.507665   \n",
              "\n",
              "            7         8         9  ...       391       392       393  \\\n",
              "0   -0.996278 -2.306744 -1.294890  ...  0.065952 -0.009361 -0.079343   \n",
              "1    1.340459 -4.633610 -0.483502  ... -0.123603  0.120566  0.191876   \n",
              "2    2.629458  1.251710  1.656291  ...  0.047435 -0.052997 -0.002885   \n",
              "3   -1.478115 -2.406238 -3.754295  ... -0.144300 -0.015915  0.102075   \n",
              "4   -0.603866 -1.312170  3.553396  ...  0.036211 -0.050934  0.042479   \n",
              "..        ...       ...       ...  ...       ...       ...       ...   \n",
              "923  2.072028  0.318600  1.423024  ...  0.006278  0.026028  0.054533   \n",
              "924  0.456509 -0.855480  0.523852  ... -0.083129  0.289920  0.110156   \n",
              "925  2.413092 -0.426080 -2.335654  ... -0.028829  0.041517  0.031588   \n",
              "926  0.469497 -0.575796 -1.181286  ...  0.052730 -0.093833  0.124342   \n",
              "927 -0.342197 -0.152731 -3.172417  ...  0.017154  0.061126 -0.064348   \n",
              "\n",
              "          394       395       396       397       398       399  target  \n",
              "0    0.130464 -0.105183  0.076634  0.052387  0.122587  0.053437       2  \n",
              "1   -0.062997  0.043188  0.164884 -0.023881  0.080277  0.083184       2  \n",
              "2    0.076438  0.031182 -0.083977  0.022708  0.064800 -0.044042       2  \n",
              "3   -0.010077 -0.036840 -0.126834  0.034503 -0.127597  0.000689       2  \n",
              "4    0.026889  0.060875 -0.038849  0.132917  0.214894 -0.152744       2  \n",
              "..        ...       ...       ...       ...       ...       ...     ...  \n",
              "923 -0.212951 -0.135672  0.091030 -0.170808 -0.020764 -0.096268       3  \n",
              "924  0.001994 -0.121359  0.126620 -0.125365 -0.019861  0.134755       3  \n",
              "925  0.046523  0.078297 -0.203419  0.159186  0.044951  0.038521       3  \n",
              "926 -0.076364  0.001446  0.065485 -0.183273  0.038104 -0.233730       3  \n",
              "927 -0.031570  0.149201  0.126805 -0.180114 -0.127870 -0.013958       3  \n",
              "\n",
              "[928 rows x 401 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#do PCA and choose componeents as 400\n",
        "pca = PCA(n_components=400)\n",
        "x_pca = pca.fit_transform(test_final)\n",
        "x_pca = pd.DataFrame(x_pca)\n",
        "\n",
        "# Calculate the variance explained by priciple components\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print('Variance of each component:', pca.explained_variance_ratio_)\n",
        "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
        "\n",
        "#store the new pca generated dimensions in a dataframe\n",
        "#store the new pca generated dimensions in a dataframe\n",
        "pca_df = pd.DataFrame(data = x_pca)\n",
        "target = pd.Series(result_df.iloc[:,-1], name='target')\n",
        "final_result_df = pd.concat([pca_df, target], axis=1)\n",
        "final_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Rfdk0yXlO6rz"
      },
      "outputs": [],
      "source": [
        "#save to dimensionally reduced csv file\n",
        "final_result_df.to_csv(\"pca_final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n18ljrh0KJB3",
        "outputId": "7a6f991c-c1e9-430e-c97c-a4d8c2542876"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['PCA_ECG.pkl']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "#save the PCA model\n",
        "joblib_file='PCA_ECG.pkl'\n",
        "joblib.dump(pca,joblib_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l93cv3DXjw2A"
      },
      "source": [
        "#### **TRYING DIFFERENT ML MODELS ON THE ALL 12 LEADS COMBINED FILE WITHOUT DIMENSIONALITY REDUCTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLAdjoV0j5oS"
      },
      "source": [
        "##### **KNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np5A0l30TYEI",
        "outputId": "9e5f4a0e-90d6-4853-a687-f4e69a70de1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.793010752688172\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.65      0.76       105\n",
            "           1       0.95      0.91      0.93        94\n",
            "           2       0.70      0.86      0.77       112\n",
            "           3       0.65      0.74      0.69        61\n",
            "\n",
            "    accuracy                           0.79       372\n",
            "   macro avg       0.80      0.79      0.79       372\n",
            "weighted avg       0.81      0.79      0.79       372\n",
            "\n",
            "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary modules for ML model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup the pipeline steps:\n",
        "steps = [('knn', KNeighborsClassifier())]\n",
        "        \n",
        "# Create the pipeline: pipeline \n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
        "k_range = list(range(1, 30))\n",
        "parameters = dict(knn__n_neighbors=k_range)\n",
        "\n",
        "#input\n",
        "X = final_result_df.iloc[:,:-1]\n",
        "\n",
        "#target\n",
        "y=final_result_df.iloc[:,-1]\n",
        "\n",
        "# Create train and test sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
        "\n",
        "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
        "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
        "\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred\n",
        "y_pred = cv.predict(X_test)\n",
        "\n",
        "Knn_Accuracy = cv.score(X_test, y_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "METf9MJdkESh"
      },
      "source": [
        "##### **LOGISTIC REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3qBayEUskDfc"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup the pipeline steps:\n",
        "steps = [('lr', LogisticRegression())]\n",
        "        \n",
        "# Create the pipeline: pipeline \n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "#input\n",
        "X = final_result_df.iloc[:,:-1]\n",
        "\n",
        "#target\n",
        "y=final_result_df.iloc[:,-1]\n",
        "\n",
        "#parameters for gridsearchcv\n",
        "c_space = np.logspace(-4, 4, 10)\n",
        "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
        "\n",
        "#call GridSearchCV and set crossvalscore to 2\n",
        "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
        "\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred\n",
        "y_pred = cv.predict(X_test)\n",
        "LR_Accuracy = cv.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xV9YlHZktj9",
        "outputId": "b6fc9558-a064-407b-ced4-c81e3aefbcee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7795698924731183\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.58      0.69       105\n",
            "           1       0.83      0.91      0.87        94\n",
            "           2       0.82      0.86      0.84       112\n",
            "           3       0.59      0.77      0.67        61\n",
            "\n",
            "    accuracy                           0.78       372\n",
            "   macro avg       0.77      0.78      0.77       372\n",
            "weighted avg       0.79      0.78      0.78       372\n",
            "\n",
            "Tuned Model Parameters: {'lr__C': 0.3593813663804626, 'lr__penalty': 'l2'}\n"
          ]
        }
      ],
      "source": [
        "# Compute and print metrics\n",
        "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOJDLUA6p5Xd"
      },
      "source": [
        "##### **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG0rz8crlJK5",
        "outputId": "a560d998-8fd8-4fe9-bac6-be22cda925b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9051724137931034\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       119\n",
            "           1       1.00      1.00      1.00       125\n",
            "           2       0.91      0.89      0.90       140\n",
            "           3       0.93      0.78      0.84        80\n",
            "\n",
            "    accuracy                           0.91       464\n",
            "   macro avg       0.91      0.89      0.90       464\n",
            "weighted avg       0.91      0.91      0.91       464\n",
            "\n",
            "Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.01}\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary modules for ML model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Setup the pipeline\n",
        "steps = [('SVM', SVC())]\n",
        "\n",
        "pipeline = Pipeline(steps)\n",
        "\n",
        "#input\n",
        "X = final_result_df.iloc[:,:-1]\n",
        "\n",
        "#target\n",
        "y=final_result_df.iloc[:,-1]\n",
        "\n",
        "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
        "#since it takes lots of time in google colab provided only a single value\n",
        "parameters = {'SVM__C':[1, 10, 100],\n",
        "              'SVM__gamma':[0.1, 0.01]}\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
        "\n",
        "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
        "cv.fit(X_train,y_train)\n",
        "\n",
        "y_pred = cv.predict(X_test)\n",
        "SVM_Accuracy = cv.score(X_test, y_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "SVM_Accuracy=cv.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxwP-4vIbbm"
      },
      "source": [
        "### **XGBOOST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4xWCZUjyIWoJ"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, objective='multi:softprob', ...)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bzdU_OL7IY0U"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "predictions = [round(value) for value in y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OY33WUdIapb",
        "outputId": "40e112ea-0807-47bc-a873-0bdeeca4f0c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8512931034482759\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.72      0.75       119\n",
            "           1       0.98      1.00      0.99       125\n",
            "           2       0.81      0.89      0.84       140\n",
            "           3       0.82      0.75      0.78        80\n",
            "\n",
            "    accuracy                           0.85       464\n",
            "   macro avg       0.85      0.84      0.84       464\n",
            "weighted avg       0.85      0.85      0.85       464\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# evaluate predictions\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy: {}\".format(accuracy))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izUvTTDBp-Vy"
      },
      "source": [
        "#### **SAVING A VERY BASIC ML MODEL AND USING IT ON REALTIME PIPELINE TO CHECK WORKING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN7RIL_3PnJb",
        "outputId": "6bcad7b1-cfbc-4066-f8c2-5404e4e6da11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['knn_model_test.pkl']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the necessary modules for ML model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import joblib\n",
        "#input\n",
        "X = final_result_df.iloc[:,:-1]\n",
        "#target\n",
        "y=final_result_df.iloc[:,-1]\n",
        "\n",
        "# Create train and test sets \n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
        "\n",
        "knn =  KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "joblib_file='knn_model_test.pkl'\n",
        "joblib.dump(knn,joblib_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNiBzgDv_-oA",
        "outputId": "e68d2bfc-1935-404e-de27-3628e2d9e0c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['svm_model_test.pkl']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the necessary modules for ML model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split,GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "#input\n",
        "X = pd.read_csv('final_1D.csv',header=None)\n",
        "\n",
        "#target\n",
        "y=final_result_df.iloc[:,-1]\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
        "\n",
        "svm=SVC(C=10,gamma=0.01)\n",
        "\n",
        "svm.fit(X_train,y_train)\n",
        "\n",
        "joblib_file='svm_model_test.pkl'\n",
        "joblib.dump(svm,joblib_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmKRxfCBH7Yo"
      },
      "source": [
        "### **ENSEMBLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7V2P-WtiYZ4f"
      },
      "outputs": [],
      "source": [
        "# Importing required modules\n",
        "from sklearn import linear_model, tree, ensemble\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "eA1TkdrcYZ4g"
      },
      "outputs": [],
      "source": [
        "#input\n",
        "X = final_result_df.iloc[:,0:-1]\n",
        "\n",
        "#target\n",
        "y=final_result_df.iloc[:,-1]\n",
        "\n",
        "# Create train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "A4caey39YZ4f"
      },
      "outputs": [],
      "source": [
        "# Stacking of ML Models\n",
        "eclf = VotingClassifier(estimators=[ \n",
        "    ('SVM', SVC(probability=True)),\n",
        "    ('knn', KNeighborsClassifier()),\n",
        "    ('rf', ensemble.RandomForestClassifier()),\n",
        "    ('bayes',GaussianNB()),\n",
        "    ('logistic',LogisticRegression()),\n",
        "    ], voting='soft')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "neqxcy3GqS91",
        "outputId": "5acb921a-9719-4836-8ef2-4b7ba370e27b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'SVM__C': 100, 'SVM__gamma': 0.1, 'knn__n_neighbors': 5, 'rf__n_estimators': 300}\n",
            "Accuracy: 0.9283154121863799\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97        80\n",
            "           1       1.00      1.00      1.00        72\n",
            "           2       0.84      0.92      0.88        79\n",
            "           3       0.86      0.79      0.83        48\n",
            "\n",
            "    accuracy                           0.93       279\n",
            "   macro avg       0.93      0.92      0.92       279\n",
            "weighted avg       0.93      0.93      0.93       279\n",
            "\n",
            "{'SVM__C': 100, 'SVM__gamma': 0.1, 'knn__n_neighbors': 5, 'rf__n_estimators': 300}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter Tuning using gridSearch\n",
        "params = {'SVM__C':[1, 10, 100],\n",
        "          'SVM__gamma':[0.1, 0.01],\n",
        "          'knn__n_neighbors': [1,3,5],\n",
        "          'rf__n_estimators':[300, 400],\n",
        "          }\n",
        "\n",
        "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
        "voting_clf = grid.fit(X_train, y_train)\n",
        "\n",
        "print(grid.best_params_)\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Compute and print metrics\n",
        "Voting_Accuracy=voting_clf.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy: {}\".format(Voting_Accuracy))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(voting_clf.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "91lBO_ZUYZ4j"
      },
      "outputs": [],
      "source": [
        "# open a file, where you ant to store the data\n",
        "file = open('Heart_Disease_Prediction_using_ECG.pkl', 'wb')\n",
        "# dump information to that file\n",
        "pickle.dump(voting_clf, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emu__tW7qP9b"
      },
      "source": [
        "## SAVE AND USE THE ABOVE MODEL IN THE STREAMLIT APP : **https://colab.research.google.com/drive/139YVmcUBCiP52J2sX3QE_eiu2sukVgpn?usp=sharing**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Merging_Scaled_1D_&_Trying_Different_CLassification_ML_Models_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
